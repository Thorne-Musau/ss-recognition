{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0e04ea-612b-432a-a519-fc5e777012b3",
   "metadata": {},
   "source": [
    "## Shooting sound Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e77bb01-bf20-44f8-a67c-ba622c7cf740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17a6c5-e8f9-4bd1-836c-769a625849b8",
   "metadata": {},
   "source": [
    "## Audio Feature Extraction Function\n",
    "\n",
    "The following Python function `extract_features` is designed to load audio files and extract Mel-frequency cepstral coefficients (MFCCs) from them. The MFCCs are commonly used features in audio processing and are particularly effective for identifying characteristics in sound, such as those required for the classification of gunshot sounds. The function takes the file path of an audio file as input, loads the audio using `librosa`, and then calculates the MFCCs with a default setting of 40 coefficients per frame and a hop length of 512 samples. The resulting MFCCs are returned for further analysis or model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ed68c2-9bd6-495d-bcc9-a78a8ae7a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and extract features from audio files\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, hop_length=512)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0530ed-b093-4ed2-b086-412fbcc07da9",
   "metadata": {},
   "source": [
    "## Loading and Preparing Audio Data\n",
    "\n",
    "Loads and extracts MFCC features from audio files in specific directories for gunshot and ambient sounds, labeling them appropriately. It then combines the features and labels, shuffles the data, and prepares it for use in a machine learning model. The final data shapes are printed to verify the successful processing of the audio files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03dfe651-433e-4284-9a4f-f4c4fb922503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load audio files from a specific directory\n",
    "def load_audio_files_from_dir(directory, label):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            mfccs = extract_features(file_path)\n",
    "            for mfcc in mfccs.T:  # Each column is a feature vector for one frame\n",
    "                features.append(mfcc)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Define the paths to your data directories using raw strings for Windows paths\n",
    "automatic_path = 'Gunshots (1)/Gunshots/Automatic'\n",
    "single_shot_path = 'Gunshots (1)/Gunshots/Single Shot'\n",
    "ambience_path = '1_Ambience Sound/Ambience Sound'\n",
    "\n",
    "# Load gunshots data from both \"Automatic\" and \"Single shot\" folders\n",
    "automatic_features, automatic_labels = load_audio_files_from_dir(automatic_path, label=1)\n",
    "single_shot_features, single_shot_labels = load_audio_files_from_dir(single_shot_path, label=1)\n",
    "\n",
    "# Combine the gunshots data from both folders\n",
    "gunshots_features = automatic_features + single_shot_features\n",
    "gunshots_labels = automatic_labels + single_shot_labels\n",
    "\n",
    "# Load ambience sound data\n",
    "ambience_features, ambience_labels = load_audio_files_from_dir(ambience_path, label=0)\n",
    "\n",
    "# Combine all features and labels\n",
    "features = gunshots_features + ambience_features\n",
    "labels = gunshots_labels + ambience_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63972e7c-d7ba-479e-908c-1a05c1feefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29951222-a5a0-40fb-946f-2f4dbbe4da29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features (X): (202713, 40)\n",
      "Shape of labels (y): (202713,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the data\n",
    "print(f'Shape of features (X): {X.shape}')\n",
    "print(f'Shape of labels (y): {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86806d-9d3c-4c9a-9222-b3ba1f7a6acc",
   "metadata": {},
   "source": [
    "### Model Training with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc64d2b-dea0-49b1-bf7d-07cf4c8181d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gunshots in the training set: 3441\n",
      "Number of ambient sounds in the training set: 158729\n",
      "Number of gunshots in the test set: 860\n",
      "Number of ambient sounds in the test set: 39683\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified splitting to ensure class balance in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print the class distribution in the training and test sets\n",
    "print(f'Number of gunshots in the training set: {np.sum(y_train == 1)}')\n",
    "print(f'Number of ambient sounds in the training set: {np.sum(y_train == 0)}')\n",
    "print(f'Number of gunshots in the test set: {np.sum(y_test == 1)}')\n",
    "print(f'Number of ambient sounds in the test set: {np.sum(y_test == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085855a0-77d9-4381-839d-d2024b71fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for CNN input: (number of samples, number of MFCCs, 1)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Convert labels to categorical (if needed, though binary here)\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94da8ae7-23b6-4b4c-925f-62636001f8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thorn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78bf6b1-1931-47e3-8eb1-92af9b0262b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0225 - val_accuracy: 0.9997 - val_loss: 0.0016\n",
      "Epoch 2/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.9996 - val_loss: 0.0018\n",
      "Epoch 3/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 0.9993 - val_loss: 0.0018\n",
      "Epoch 4/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9997 - val_loss: 0.0011\n",
      "Epoch 5/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 0.9997 - val_loss: 0.0013\n",
      "Epoch 6/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9997 - val_loss: 0.0013\n",
      "Epoch 7/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9971 - val_loss: 0.0205\n",
      "Epoch 8/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.9995 - val_loss: 0.0017\n",
      "Epoch 9/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9997 - val_loss: 0.0011\n",
      "Epoch 10/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.9997 - val_loss: 0.0013\n",
      "Epoch 11/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9997 - val_loss: 0.0015\n",
      "Epoch 12/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9996 - val_loss: 0.0020\n",
      "Epoch 13/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9997 - val_loss: 9.3277e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9997 - val_loss: 0.0012\n",
      "Epoch 15/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9995 - val_loss: 0.0019\n",
      "Epoch 16/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9997 - val_loss: 0.0012\n",
      "Epoch 17/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9996 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9997 - val_loss: 8.3405e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 8.9267e-04 - val_accuracy: 0.9996 - val_loss: 0.0016\n",
      "Epoch 20/20\n",
      "\u001b[1m5068/5068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9995 - val_loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3e4ccb-259e-408d-875b-0cc96dc51bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0017\n",
      "Test accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bbe736d-9e93-473d-970e-f201e787fa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "[[39672    11]\n",
      " [    8   852]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39683\n",
      "           1       0.99      0.99      0.99       860\n",
      "\n",
      "    accuracy                           1.00     40543\n",
      "   macro avg       0.99      1.00      0.99     40543\n",
      "weighted avg       1.00      1.00      1.00     40543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886dfa8d-a428-400a-a8f9-a8e192ff79c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gunshots in the test set: 860\n",
      "Number of ambient sounds in the test set: 39683\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of gunshots in the test set: {np.sum(y_test == 1)}')\n",
    "print(f'Number of ambient sounds in the test set: {np.sum(y_test == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b3251-68be-4d0d-90d9-2b4b20019521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
